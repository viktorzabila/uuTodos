const sizeOf = require("object-sizeof");
const pump = require("pump");
const s = require("stream");
const { Transform } = s;
const LineByLineReader = require("line-by-line");
const { EJSON, Binary } = require("bson");
const JSZIP = require("jszip");
const unzip = require("unzip-stream");
const DeepMerge = require("deepmerge");

const { BaseError, Config } = require("uu_appg01_core-utils");
const { LoggerFactory } = require("uu_appg01_core-logging");
const { Validator } = require("uu_appg01_core-validation");
const { ValidationHelper, Sys } = require("uu_appg01_core-appserver");
const { DaoFactory, ObjectStoreError } = require("uu_appg01_objectstore");

const SysAppInstanceAbl = require("./sys-app-instance-abl");
const SysAppWorkspaceAbl = require("./sys-app-workspace-abl");
const SysDataStoreStatsAbl = require("./sys-app-data-store-stats-abl");
const SysDumpRestoreMongo = require("../dao/sys-dump-restore-mongo");
const Errors = require("../api/errors/sys-dump-restore-errors");
const WorkspaceError = require("../api/errors/workspace-error");
const ImportMetadata = require("../helpers/import-metadata");

const logger = LoggerFactory.get("UuApp.Workspace.Abl.SysDumpRestoreAbl");
const MAXIMUM_DUMP_SIZE = Config.get("uu_app_workspace_maximum_dump_size") || 1024 * 1024 * 1024;
const STATUS_FILE = "status.json";
const WARNINGS = {
  importUnsupportedKeys: `${Errors.SysImport.UC_CODE}unsupportedKeys`,
  importIndexImportFailed: `${Errors.SysImport.UC_CODE}indexImportFailed`,
  importConflictingProfiles: `${Errors.SysImport.UC_CODE}conflictingProfiles`,
  listMappingsUnsupportedKeys: `${Errors.SysListImportMappings.UC_CODE}unsupportedKeys`
};
const QUEUE_LIMIT = 32000; //32KB
const IMPORT_MAPPING_SCHEMA = "sysImportMapping";

class SysDumpRestoreAbl {
  constructor() {
    this.validator = Validator.load();
    this.importMappingDao = DaoFactory.getDao(IMPORT_MAPPING_SCHEMA);
  }

  async import(awid, dtoIn) {
    // HDS 1. - Performs a logical check of dtoIn.
    let validationResult = this.validator.validate("sysImportDtoInType", dtoIn);
    let uuAppErrorMap = ValidationHelper.processValidationResult(
      dtoIn,
      validationResult,
      WARNINGS.importUnsupportedKeys,
      Errors.SysImport.InvalidDtoIn
    );

    // HDS 2. - Loads a sysAppWorkspace uuObject (sysAppWorkspace DAO getByAwid) (A3)
    let appWorkspace;
    try {
      appWorkspace = await SysAppWorkspaceAbl.get(awid);
    } catch (e) {
      if (e instanceof WorkspaceError) {
        // A3 - sysAppWorkspace uuObject does not exist
        throw new Errors.SysImport.SysUuAppWorkspaceDoesNotExist({}, { awid });
      }
      throw e;
    }

    // HDS 3. - Begins to process dtoIn.data (i.e. dump) as a stream
    await new Promise((resolve, reject) => {
      let unzipStream = pump(dtoIn.data, unzip.Parse());
      let entryPromise; // indicates whether last entry was processed so dtoOut returns after restoring all data
      let deletedBinaryRegister = [];
      let fileIdMapping = {};

      unzipStream.on("entry", async entry => {
        // HDS 4. - (*) For every file in dtoIn.data (i.e. dump of a single database schema) makes the following:
        entryPromise = new Promise(async resolve => {
          try {
            unzipStream.pause();
            entryPromise = await this._importSchema(
              entry,
              uuAppErrorMap,
              awid,
              appWorkspace,
              deletedBinaryRegister,
              fileIdMapping
            );
            unzipStream.resume();
            resolve();
          } catch (e) {
            unzipStream.destroy(e);
          }
        });
      });

      unzipStream.on("error", e => {
        if (e instanceof WorkspaceError) {
          reject(e);
        } else {
          reject(new Errors.SysImport.ImportFailed({ uuAppErrorMap }, { cause: e.message }, e));
        }
      });

      unzipStream.on("end", async () => {
        if (entryPromise) {
          await entryPromise;
        }
        resolve();
      });
    });

    //clear caches
    SysAppWorkspaceAbl.clearCache();

    // HDS 5. - Return correctly filled out dtoOut.
    return { uuAppErrorMap };
  }
  /**
   * Deletes ObjectIds relations (created in sys/uuAppWorkspace/import) for one awid.
   *
   * @param {String} awid
   * @param {Object} uuAppErrorMap
   * @returns {Promise<*>}
   */
  async deleteImportMappings(awid, uuAppErrorMap = {}) {
    // HDS 1. - Deletes records from database. (sysImportMapping DAO deleteByAwid).
    await this.importMappingDao.delete(awid);

    // HDS 2. - Returns properly filled out dtoOut.
    return { uuAppErrorMap: uuAppErrorMap };
  }
  /**
   * List ObjectId mappings created by sys/uuAppWorkspace/import.
   *
   * @param {String} awid
   * @param {Object} dtoIn
   * @returns {Promise<*>}
   */
  async listImportMappings(awid, dtoIn) {
    // HDS 1. - Performs a logical check of dtoIn. (A1, A2)
    let validationResult = this.validator.validate("sysListImportMappingsDtoInType", dtoIn);
    let uuAppErrorMap = ValidationHelper.processValidationResult(
      dtoIn,
      validationResult,
      WARNINGS.listMappingsUnsupportedKeys,
      Errors.SysListImportMappings.InvalidDtoIn
    ); // A1, A2

    // HDS 2. - Loads records from database.
    let list;
    if (dtoIn.origIdList) {
      // HDS 2.1. - If there is dtoIn.origIdList in input, system loads records from the list.
      // (sysImportMapping DAO listByOrigIdList)
      list = await this.importMappingDao.listByOrigIdList(awid, dtoIn.origIdList, dtoIn.pageInfo || {});
    } else {
      // HDS 2.2 - If dtoIn.origIdList is missing, system loads all records. (sysImportMapping DAO list)
      list = await this.importMappingDao.list(awid, dtoIn.pageInfo || {});
    }

    // HDS 3. - Returns properly filled out dtoOut.
    list.uuAppErrorMap = uuAppErrorMap;
    return list;
  }

  /**
   * Return dump of database as a stream
   * @param dtoIn
   * @returns {Promise<*>}
   */
  async dump(dtoIn) {
    //HDS 1, A2, A3
    let validationResult = this.validator.validate("sysDumpDtoInType", dtoIn);
    let uuAppErrorMap = ValidationHelper.processValidationResult(
      dtoIn,
      validationResult,
      `${Errors.SysDump.UC_CODE}unsupportedKeys`,
      Errors.SysDump.InvalidDtoInError
    );

    //HDS 2
    let awidList = dtoIn.awidList || [];

    //HDS 2.1
    let stats = await SysDataStoreStatsAbl.list({ awidList });

    //HDS 2.2
    let totalDumpSize = 0;
    let deprecatedStats = false;
    stats.itemList.forEach(item => {
      //HDS 2.2.1
      totalDumpSize += item.size;
      if (new Date() - item.calcEndTs > 1000 * 3600 * 24 * 10) {
        deprecatedStats = true;
      }
    });
    if (stats.itemList.length === 0) {
      deprecatedStats = true;
    }

    //HDS 2.2.2
    if (deprecatedStats) {
      let dao = DaoFactory.getDaos()
        .values()
        .next().value;
      let dumpDao = this._getDumpDao(dao);
      totalDumpSize = await dumpDao.calculateDatabaseStats();
    }

    //HDS 2.3, A4
    if (totalDumpSize > MAXIMUM_DUMP_SIZE) {
      if (deprecatedStats) {
        throw new Errors.SysDump.MaximumSizeExceededNoStats(
          { uuAppErrorMap },
          {
            maxSize: MAXIMUM_DUMP_SIZE,
            size: totalDumpSize
          }
        );
      } else {
        throw new Errors.SysDump.MaximumSizeExceeded(
          { uuAppErrorMap },
          {
            maxSize: MAXIMUM_DUMP_SIZE,
            size: totalDumpSize
          }
        );
      }
    }

    //HDS 3
    let archive = new JSZIP();

    //HDS 4, 5
    await this._exportSchemas(awidList, archive);

    //HDS 6
    return await archive.generateNodeStream({ type: "nodebuffer", streamFiles: true, compression: "DEFLATE" });
  }

  /**
   * Restore database from dump
   * @param dtoIn
   * @param isAwidRestoreCall
   * @returns {Promise<{uuAppErrorMap: {}}>}
   */
  async restore(dtoIn, isAwidRestoreCall = false) {
    let toDeleteFileIdList = new Map();

    //HDS 1, A1, A2
    let validationResult = this.validator.validate("sysRestoreDtoInType", dtoIn);
    let uuAppErrorMap = ValidationHelper.processValidationResult(
      dtoIn,
      validationResult,
      `${Errors.SysRestore.UC_CODE}unsupportedKeys`,
      Errors.SysRestore.InvalidDtoInError
    );
    let isDroppedDatabase = false;

    //HDS 2
    await new Promise((resolve, reject) => {
      let unzipStream = pump(dtoIn.data, unzip.Parse());
      let entryPromise; // indicates whether last entry was processed so dtoOut returns after restoring all data
      let awidList = dtoIn.awidList || [];
      let isStrictMode = true;
      if (dtoIn.strictMode != null) {
        isStrictMode = dtoIn.strictMode;
      }

      //HDS 3
      unzipStream.on("entry", async entry => {
        entryPromise = new Promise(async resolve => {
          try {
            unzipStream.pause();
            let response = await this._restoreSchema(
              entry,
              awidList,
              isStrictMode,
              isDroppedDatabase,
              uuAppErrorMap,
              toDeleteFileIdList,
              isAwidRestoreCall
            );
            isDroppedDatabase = response.isDroppedDatabase;
            toDeleteFileIdList = response.toDeleteFileIdList;
            unzipStream.resume();
            resolve();
          } catch (e) {
            unzipStream.destroy(e);
          }
        });
      });

      unzipStream.on("error", e => {
        if (e instanceof WorkspaceError) {
          reject(e);
        } else {
          reject(new Errors.SysRestore.RestoreFailed({ uuAppErrorMap }, { cause: e.message }, e));
        }
      });

      unzipStream.on("end", async () => {
        if (entryPromise) {
          await entryPromise;
        }
        resolve();
      });
    });

    //clear caches
    SysAppWorkspaceAbl.clearCache();
    SysAppInstanceAbl.clearCache();

    //HDS 4
    return { uuAppErrorMap };
  }

  /**
   * Return dump of database as a stream
   * use awid from request uri
   * @returns {Promise<*>}
   * @param awid
   */
  async awidDump(awid) {
    let dtoIn = {
      awidList: [awid]
    };

    //HDS 1, HDS 2, A1
    return this.dump(dtoIn);
  }

  /**
   * Restore database from dump
   * @param awid
   * @param dtoIn
   * @returns {Promise<{uuAppErrorMap: {}}>}
   */
  async awidRestore(awid, dtoIn) {
    //HDS 1, A1, A2
    let validationResult = this.validator.validate("sysAwidRestoreDtoInType", dtoIn);
    let uuAppErrorMap = ValidationHelper.processValidationResult(
      dtoIn,
      validationResult,
      `${Errors.SysAwidRestore.UC_CODE}unsupportedKeys`,
      Errors.SysAwidRestore.InvalidDtoInError
    );

    dtoIn.awidList = [awid];

    //HDS 2, A3
    let response = await this.restore(dtoIn, true);
    response.uuAppErrorMap = DeepMerge(uuAppErrorMap, response.uuAppErrorMap);

    //HDS 3
    return response;
  }

  /**
   * Restore schema
   * @param stream
   * @param awidList
   * @param isStrictMode
   * @param isDroppedDatabase
   * @param uuAppErrorMap
   * @param toDeleteFileIdList
   * @param isAwidRestoreCall
   * @returns {Promise<{isDroppedDatabase: *, toDeleteFileIdList: *}>}
   * @private
   */
  async _restoreSchema(
    stream,
    awidList,
    isStrictMode,
    isDroppedDatabase,
    uuAppErrorMap,
    toDeleteFileIdList,
    isAwidRestoreCall
  ) {
    let schema = stream.path.substring(0, stream.path.lastIndexOf("."));

    if (schema === "status") {
      return { isDroppedDatabase, toDeleteFileIdList };
    }

    let metadataLine = true;
    let bulkQueue = [];
    let indexList = [];
    let metadataAwidList = {};
    let progressList = {};

    let dao;
    await this._eachLine(stream, async (line, lineNumber) => {
      let parsedLine = null;

      //HDS 3.7.1
      try {
        parsedLine = EJSON.parse(line);
      } catch (e) {
        if (metadataLine) {
          // A3
          throw new Errors.SysRestore.InvalidSchemaMetadata({ uuAppErrorMap }, { schema }, e);
        } else if (isStrictMode) {
          // A8.1
          throw new Errors.SysRestore.InvalidDump({ uuAppErrorMap }, { schema, lineNumber, cause: e.message }, e);
        } else {
          // A8.2
          logger.error(
            `Dump row is not valid, strict mode set to false -> continuing the restore process. Schema: ${schema}, lineNuber: ${lineNumber}.`,
            e
          );

          let invalidRowErrorCode;
          if (isAwidRestoreCall) {
            invalidRowErrorCode = new Errors.SysAwidRestore.InvalidDumpRow().code;
          } else {
            invalidRowErrorCode = new Errors.SysRestore.InvalidDumpRow().code;
          }

          if (!uuAppErrorMap[invalidRowErrorCode]) {
            ValidationHelper.addWarning(
              uuAppErrorMap,
              invalidRowErrorCode,
              "Dump row is not valid. See the cause for more details.",
              {
                rows: [
                  {
                    schema,
                    lineNumber,
                    cause: e.message
                  }
                ]
              }
            );
          } else {
            uuAppErrorMap[invalidRowErrorCode].paramMap.rows.push({ schema, cause: e.message });
          }
        }
      }
      if (parsedLine !== null) {
        if (metadataLine) {
          //HDS 3.1, 3.2, 3.3, 3.4
          parsedLine = JSON.parse(line);
          await this._validateMetadata(parsedLine, awidList, schema);
          schema = parsedLine.schemaName;

          ///HDS 3.5
          dao = await this._getRestoreDao(schema, uuAppErrorMap);

          //HDS 3.6
          if (!isDroppedDatabase) {
            let response = await this._deleteAwidDocuments(awidList, parsedLine.awidList, dao, toDeleteFileIdList);
            isDroppedDatabase = response.isOnlyAsid;
            response.newDeletedFileIdList.forEach(fileId => {
              toDeleteFileIdList.set(fileId, true);
            });
          }

          parsedLine.awidList.forEach(awidObject => {
            if (awidObject.awid) {
              metadataAwidList[awidObject.awid] = awidObject;
            } else {
              metadataAwidList[awidObject.asid] = awidObject;
            }
          });

          progressList = this._createProgressList(metadataAwidList, schema);
          indexList = parsedLine.indexList;

          metadataLine = false;
        } else {
          //HDS 3.7.2
          let written = await this._processDataLine(
            parsedLine,
            toDeleteFileIdList,
            metadataAwidList,
            bulkQueue,
            awidList,
            dao,
            isStrictMode,
            progressList,
            schema,
            uuAppErrorMap
          );
          if (written) {
            bulkQueue = [];
          }
        }
      }
    });

    if (dao) {
      //HDS 3.8
      await this._bulkWrite(
        dao,
        bulkQueue,
        isStrictMode,
        schema,
        progressList,
        toDeleteFileIdList,
        uuAppErrorMap,
        true
      );
      bulkQueue = [];

      //HDS 3.9, A10
      let dumpDao = this._getDumpDao(dao, schema);
      let isAsid = awidList.length === 1 && awidList[0] === Config.get("asid");
      await dumpDao.createIndexes(indexList, isAsid, schema, uuAppErrorMap);
    }

    return { isDroppedDatabase, toDeleteFileIdList };
  }

  /**
   * Process data line
   * @param parsedLine
   * @param toDeletedFileIdList
   * @param metadataAwidList
   * @param bulkQueue
   * @param awidList
   * @param dao
   * @param isStrictMode
   * @param progressList
   * @param schema
   * @param uuAppErrorMap
   * @returns {Promise<boolean>}
   * @private
   */
  async _processDataLine(
    parsedLine,
    toDeletedFileIdList,
    metadataAwidList,
    bulkQueue,
    awidList,
    dao,
    isStrictMode,
    progressList,
    schema,
    uuAppErrorMap
  ) {
    let checkBinaryForFileId = dao.collectionName.endsWith(".files") || dao.collectionName.endsWith(".chunks");
    let isAsid = Object.keys(metadataAwidList).length === 1 && Object.keys(metadataAwidList)[0] === Config.get("asid");

    //HDS 3.7.2
    if (checkBinaryForFileId && !isAsid) {
      let awidOfObject;
      let foundFileId = false;
      let keys = Object.keys(metadataAwidList);
      for (let i = 0; i < keys.length; i += 1) {
        let awidObject = metadataAwidList[keys[i]];
        if (awidList.length > 0 && !awidList.includes(awidObject.awid)) {
          continue;
        }
        let idToCheck;
        if (dao.collectionName.endsWith(".chunks")) {
          idToCheck = parsedLine.files_id.toString();
        } else {
          idToCheck = parsedLine._id.toString();
        }
        if (awidObject.fileIdList && awidObject.fileIdList.includes(idToCheck)) {
          foundFileId = true;
          awidOfObject = keys[i];
          break;
        }
      }
      if (foundFileId) {
        //HDS 3.7.3
        bulkQueue.push({ awid: awidOfObject, data: parsedLine });
      }
    } else if (awidList.length > 0 && awidList.includes(parsedLine.awid)) {
      //HDS 3.7.3
      bulkQueue.push({ awid: parsedLine.awid, data: parsedLine });
    } else if (awidList.length === 0) {
      //HDS 3.7.3
      bulkQueue.push({ awid: parsedLine.awid, data: parsedLine });
    }

    return await this._bulkWrite(
      dao,
      bulkQueue,
      isStrictMode,
      schema,
      progressList,
      toDeletedFileIdList,
      uuAppErrorMap
    );
  }

  /**
   * Execute bulk write to the database
   * @param dao
   * @param bulkQueue
   * @param isStrictMode
   * @param schema
   * @param progressList
   * @param toDeleteFileIdList
   * @param uuAppErrorMap
   * @param force
   * @private
   */
  async _bulkWrite(
    dao,
    bulkQueue,
    isStrictMode,
    schema,
    progressList,
    toDeleteFileIdList,
    uuAppErrorMap,
    force = false
  ) {
    //HDS 3.7.4 A9
    if (sizeOf(bulkQueue) > QUEUE_LIMIT || force) {
      let result;
      try {
        result = await dao.insertMany(bulkQueue);
      } catch (e) {
        if (isStrictMode) {
          throw new Errors.SysRestore.RestoreFailed({ uuAppErrorMap }, { schema, cause: e.message }, e);
        } else {
          let invalidRows = [];
          let errorResult = e.result;
          let nInserted = errorResult.toJSON()["nInserted"];
          let insertedIds = errorResult.toJSON().insertedIds.slice(nInserted);
          insertedIds.forEach(insertedId => {
            invalidRows.push({ schema, objectId: insertedId._id, cause: e.message });
          });
          if (!uuAppErrorMap["rowRestoreFailed"]) {
            ValidationHelper.addWarning(
              uuAppErrorMap,
              "rowRestoreFailed",
              "Insert by DAO failed. See the cause for more details.",
              { rows: invalidRows }
            );
          } else {
            uuAppErrorMap["rowRestoreFailed"].paramMap.rows.push(...invalidRows);
          }
        }
      }
      if (result && result.insertedIds) {
        let ids = [];
        Object.keys(result.insertedIds).forEach(index => {
          let id = result.insertedIds[index];
          ids.push(id);
        });
        bulkQueue.forEach(doc => {
          if (doc.data.fileId != null && ids.concat(doc.data._id.toString())) {
            toDeleteFileIdList.set(doc.data.fileId.toString(), false);
          }
        });
      }
      //HDS 3.7.5
      this._processProgress(bulkQueue, progressList);
      return true;
    } else {
      return false;
    }
  }

  /**
   * Create progress list with count of items and count of already processed items
   * @param metadataAwidList
   * @param schema
   * @private
   */
  _createProgressList(metadataAwidList, schema) {
    let progressList = {};
    Object.keys(metadataAwidList).forEach(key => {
      let awidObject = metadataAwidList[key];
      if (awidObject.awid) {
        progressList[awidObject.awid] = { itemCount: awidObject.itemCount, processed: 0, isAsid: false };
        progressList[awidObject.awid].schema = schema;
      } else if (awidObject.asid) {
        progressList[awidObject.asid] = { itemCount: awidObject.itemCount, processed: 0, isAsid: true };
        progressList[awidObject.asid].schema = schema;
      }
    });
    return progressList;
  }

  /**
   * Log progress of each awids
   * @param bulkQueue
   * @param progressList
   * @private
   */
  _processProgress(bulkQueue, progressList) {
    if (bulkQueue.length > 0) {
      bulkQueue.forEach(document => {
        if (document.awid) {
          if (!progressList[document.awid]) {
            //only asid in progressList
            progressList[Object.keys(progressList)[0]].processed += 1;
          } else {
            progressList[document.awid].processed += 1;
          }
        }
      });

      Object.keys(progressList).forEach(key => {
        let itemCount = progressList[key]["itemCount"].value;
        let processed = progressList[key]["processed"];
        let schema = progressList[key].schema;

        let awidOrAsid = progressList[key].isAsid ? "asid" : "awid";

        if (processed === itemCount) {
          logger.info(`${processed} of ${itemCount} was processed of ${awidOrAsid} ${key} for schema: ${schema}`);
        } else {
          for (let i = 1; i <= 4; i += 1) {
            if (processed / itemCount > i * 0.195 && processed / itemCount < i * 0.201) {
              logger.info(`${processed} of ${itemCount} was processed of ${awidOrAsid} ${key} for schema: ${schema}`);
            }
          }
        }
      });
    }
  }

  /**
   * Validate metadata line
   * @param metadata
   * @param awidList
   * @param schemaFilename
   * @returns {Promise<void>}
   * @private
   */
  async _validateMetadata(metadata, awidList, schemaFilename) {
    //HDS 3.1 A3
    let validationResult = this.validator.validate("uuAppDataStoreDumpSchemaMetadataType", metadata);
    try {
      ValidationHelper.processValidationResult(
        metadata,
        validationResult,
        `${Errors.SysRestore.UC_CODE}unsupportedKeys`,
        Errors.SysRestore.InvalidSchemaMetadata
      );
    } catch (e) {
      e.paramMap.schema = metadata.schemaName || schemaFilename;
      throw e;
    }
    //HDS 3.2, 3.3
    for (let awid of awidList) {
      //A4
      let found = false;
      metadata.awidList.forEach(list => {
        if (list.awid === awid) {
          found = true;
        }
      });
      if (!found) {
        throw new Errors.SysRestore.AwidNotFoundInDump(
          {},
          {
            awid,
            dumpAwidList: metadata.awidList.map(o => o.awid) || [],
            dumpAsidList: metadata.awidList.map(o => o.asid) || []
          }
        );
      }

      //A5
      let sysAppWorkspace;
      try {
        sysAppWorkspace = await SysAppWorkspaceAbl.get(awid);
      } catch (e) {
        // there is not this workspace
      }
      if (sysAppWorkspace && sysAppWorkspace.artifactUri) {
        throw new Errors.SysRestore.artifactConnectedToRestoredAwid(
          {},
          { awid, artifactUri: sysAppWorkspace.artifactUri }
        );
      }
    }

    //HDS 3.4 A6
    if (
      awidList.length === 0 &&
      metadata.awidList[0] &&
      metadata.awidList[0].asid &&
      metadata.awidList[0].asid !== Config.get("asid")
    ) {
      throw new Errors.SysRestore.AsidNotFoundInDump(
        {},
        { asid: Config.get("asid"), dumpAwidList: metadata.awidList.map(o => o.awid) }
      );
    }
  }

  /**
   * Return dao for restore process
   * @param schema
   * @param uuAppErrorMap
   * @returns {Promise<SysDumpRestoreMongo>}
   * @private
   */
  async _getRestoreDao(schema, uuAppErrorMap) {
    let dao;
    try {
      dao = DaoFactory.getDao(schema);
      dao.isBinary = Object.getPrototypeOf(dao.constructor).name === "UuBinaryDao";
    } catch (e) {
      let binarySchema = schema.match(/(.+)\.(chunks|files)$/);
      if (binarySchema) {
        dao = await this._getRestoreDao(binarySchema[1]);
      } else {
        //A7
        throw new Errors.SysRestore.DaoNotFound({ uuAppErrorMap }, { schema });
      }
    }
    return this._getDumpDao(dao, schema);
  }

  /**
   * Iterate whole each line
   * @param stream
   * @param cb
   * @returns {Promise<*>}
   * @private
   */
  async _eachLine(stream, cb) {
    let rl = new LineByLineReader(stream);
    let lineNumber = 0;

    return new Promise((resolve, reject) => {
      rl.on("line", async line => {
        try {
          rl.pause();
          await cb(line, lineNumber++);
          rl.resume();
        } catch (e) {
          reject(e);
          rl.close(e);
        }
      });

      rl.on("end", () => {
        resolve();
      });
    });
  }

  /**
   * Delete documents for specified awids or whole database
   * @param awidList
   * @param metadataAwidList
   * @param dao
   * @param toDeletedFileIdList
   * @returns {Promise<{isOnlyAsid: boolean, deletedFileIdList: *}>}
   * @private
   */
  async _deleteAwidDocuments(awidList, metadataAwidList, dao, toDeletedFileIdList) {
    //HDS 3.6.1
    let isOnlyAsid = false;
    let isAsidInInputAwidList = awidList.length === 1 && awidList[0] === Config.get("asid");
    let isAsidInMetadataAwidList = metadataAwidList.length === 1 && metadataAwidList[0].asid != null;

    //HDS 3.6.2
    if (isAsidInMetadataAwidList || isAsidInInputAwidList) {
      isOnlyAsid = true;
    }

    let newDeletedFileIdList = [];
    if (isOnlyAsid) {
      await SysDumpRestoreMongo.dropAllDatabases();
    } else {
      let dumpDao = this._getDumpDao(dao);
      let filter;
      if (awidList && awidList.length > 0) {
        filter = { awid: { $in: awidList } };
      } else {
        filter = { awid: { $in: metadataAwidList.map(obj => obj.awid) } };
      }

      //HDS 3.6.3
      if (!dao.isBinary) {
        //HDS 3.6.3.1
        if (dao.collectionName.endsWith(".files") || dao.collectionName.endsWith(".chunks")) {
          let schema = dao.collectionName.substring(0, dao.collectionName.lastIndexOf("."));
          newDeletedFileIdList = await dumpDao.deleteBinary(filter, schema, toDeletedFileIdList);
        } else {
          await dumpDao.deleteMany(filter);
        }
      } else {
        //HDS 3.6.3.2
        newDeletedFileIdList = await dumpDao.deleteBinary(filter, dao.collectionName, toDeletedFileIdList);
      }
    }

    return { isOnlyAsid, newDeletedFileIdList };
  }

  /**
   * Fill stream with exports of schemas converted to JSON.
   * @param awidList
   * @param archive
   * @return {Promise<void>}
   * @private
   */
  async _exportSchemas(awidList, archive) {
    let startTime = new Date().toISOString();

    //HDS 5.1
    try {
      for (let dao of DaoFactory.getDaos().values()) {
        let dumpDao = this._getDumpDao(dao);

        //HDS 5.2
        let searchQuery = this._createSearchQuery(dumpDao, awidList);
        await this._saveSchema(searchQuery, dumpDao, archive, awidList);
      }

      this._saveStatus(archive, "finished", startTime);
    } catch (e) {
      logger.error("Export failed.", e);
      this._saveStatus(archive, "error", startTime, e);
    }
  }

  /**
   * Create search query for database
   * @param dumpDao
   * @param awidList
   * @returns {*}
   * @private
   */
  _createSearchQuery(dumpDao, awidList) {
    if (!awidList || (Array.isArray(awidList) && awidList.length === 0)) {
      return {};
    }

    const filter = Array.isArray(awidList) ? { $in: awidList } : awidList;
    if (dumpDao.isAsid) {
      return { asid: filter };
    } else {
      return { awid: filter };
    }
  }

  /**
   * Add all documents of DAO to stream. If Dao is binary, add also its data (.files and .chunks)
   * @param filter
   * @param dao
   * @param archive
   * @param awidList
   * @return {Promise<void>}
   * @private
   */
  async _saveSchema(filter, dao, archive, awidList) {
    const schema = dao.collectionName;
    const binaryFilesIds = [];

    const stringifyStream = new Transform({
      writableObjectMode: true,
      transform(doc, opts, callback) {
        let chunk;
        if (opts && opts.metadata === true) {
          chunk = `${JSON.stringify(doc)}\n`;
        } else {
          if (schema.endsWith(".chunks") && doc.data && doc.data._bsontype === "Binary") {
            doc.data = new Binary(doc.data.buffer, doc.data.sub_type);
          }
          chunk = `${EJSON.stringify(doc)}\n`;
          dao.isBinary && binaryFilesIds.push(doc.fileId);
        }
        this.push(chunk);
        callback();
      }
    });

    let metadata = await this._loadMetadata(awidList, dao, schema);

    stringifyStream.write(metadata, { metadata: true });

    let documents = await dao.find(filter);
    pump(documents, stringifyStream);

    archive.file(`${schema}.json`, stringifyStream);

    if (dao.isBinary) {
      let filesDao = this._getDumpDao(dao, `${schema}.files`);
      await this._saveSchema({ _id: { $in: binaryFilesIds } }, filesDao, archive, awidList);

      let chunksDao = this._getDumpDao(dao, `${schema}.chunks`);
      await this._saveSchema({ files_id: { $in: binaryFilesIds } }, chunksDao, archive, awidList);
    }
  }

  /**
   * Load metadata
   * @param awidList
   * @param dao
   * @param schema
   * @returns {Promise<{schemaName: *, awidList: Array, cts: string, uuSubAppVersion: string}>}
   * @private
   */
  async _loadMetadata(awidList, dao, schema) {
    let awidListWithCount = [];
    let isAsid = false;
    let asid;
    let list = [];

    if (awidList.length === 0) {
      asid = Config.get("asid");
      list.push(asid);
      isAsid = true;
    } else {
      list = awidList;
    }

    let promises = list.map(async awid => {
      let resultObject;
      let awidObject = {};

      if (isAsid) {
        resultObject = await this._loadCountsAndFileIdList(dao);
        awidObject.asid = asid;
      } else {
        resultObject = await this._loadCountsAndFileIdList(dao, { awid });
        awidObject.awid = awid;
      }

      awidObject.itemCount = resultObject.itemCount;

      let shouldHaveFileIdList = dao.collectionName.endsWith(".files") || dao.collectionName.endsWith(".chunks");
      if (shouldHaveFileIdList && !isAsid) {
        awidObject.fileIdList = resultObject.fileIdList;
      }

      awidListWithCount.push(awidObject);
    });

    await Promise.all(promises);

    let metadata = {
      schemaName: schema,
      awidList: awidListWithCount,
      cts: new Date().toISOString(),
      uuSubAppVersion: Sys.getAppInfo().uuSubAppVersion
    };

    try {
      let indexes = await dao.getIndexes();
      metadata.indexList = await indexes.toArray();
    } catch (e) {
      if (e.code !== 26) {
        // 26 ~ collection not initialized
        throw new BaseError("Error during export of indexes, collection: " + schema, e);
      }
      metadata.indexList = [];
    }

    return metadata;
  }

  /**
   * Load counts and fileIdList for specif schema
   * @param dao
   * @param filter
   * @returns {Promise<{itemCount: (*|number), fileIdList: Array, awid: (awid|{$in})}>}
   * @private
   */
  async _loadCountsAndFileIdList(dao, filter = {}) {
    let fileIdList = [];
    let itemCount;

    if (dao.collectionName.endsWith(".files") || dao.collectionName.endsWith(".chunks")) {
      let files = await dao.getFileIdList(filter);
      files.forEach(file => {
        fileIdList.push(file.fileId);
      });
      if (dao.collectionName.endsWith(".files")) {
        itemCount = fileIdList.length;
      } else {
        itemCount = await dao.binaryCount(fileIdList);
      }
    } else {
      itemCount = await dao.count(filter);
    }

    return {
      itemCount,
      fileIdList,
      awid: filter.awid
    };
  }

  /**
   * Create status.json file
   * @param archive
   * @param status
   * @param startTime
   * @param uuAppErrorMap
   * @private
   */
  _saveStatus(archive, status, startTime, uuAppErrorMap = {}) {
    let dumpStatus = {
      status,
      startTime,
      uuAppErrorMap,
      endTime: new Date().toISOString()
    };

    archive.file(STATUS_FILE, JSON.stringify(dumpStatus));
  }

  /**
   * Return DumpDao
   * @param dao
   * @param name
   * @returns {SysDumpRestoreMongo}
   * @private
   */
  _getDumpDao(dao, name) {
    let collectionName = name || dao.collectionName;
    let dumpDao = new SysDumpRestoreMongo(collectionName, null, null, dao.customUri);
    dumpDao.isBinary = Object.getPrototypeOf(dao.constructor).name === "UuBinaryDao";
    dumpDao.isAsid = dao.getExtraAttributes().asid === true;

    return dumpDao;
  }

  async _importSchema(entry, uuAppErrorMap, newAwid, appWorkspace, deletedBinaryRegister, fileIdMapping) {
    if (entry.path === STATUS_FILE) {
      return;
    }

    let metadata, dumpDao;
    let lastLoggedAt = 0;
    let restored = 0;
    let documentQueue = [];
    let mappingsQueue = [];

    await this._eachLine(entry, async (line, lineNumber) => {
      // HDS 4.1 - (*) For first row of the dump (i.e. schema metadata):
      if (lineNumber === 0) {
        // HDS 4.1.1. - Validates schema metadata against sysImportSchemaMetadataType (A4)
        try {
          metadata = JSON.parse(line);
        } catch (e) {
          // A4 - metadata is not valid
          throw new Errors.SysImport.InvalidSchemaMetadata({ uuAppErrorMap }, { cause: e }, e);
        }
        let validationResult = this.validator.validate("sysImportSchemaMetadataType", metadata);
        if (!validationResult.isValid()) {
          // A4 - metadata is not valid
          throw new Errors.SysImport.InvalidSchemaMetadata(
            { uuAppErrorMap },
            { schema: metadata.schemaName, ...validationResult.getValidationErrorMap() }
          );
        }
        metadata = ImportMetadata.parse(metadata);

        if (metadata.getSchema() === IMPORT_MAPPING_SCHEMA) {
          return;
        }

        // HDS 4.1.2. - Obtains DAO according to metadata.schemaName (A5)
        let daoSchemaName = metadata.getBinarySchema() || metadata.getSchema();
        let dao;
        try {
          dao = DaoFactory.getDao(daoSchemaName);
        } catch (e) {
          // A5 - DAO for schema defined in metadata was not found
          throw new Errors.SysImport.DaoNotFound({ uuAppErrorMap }, { schema: metadata.getSchema() });
        }
        dumpDao = this._getDumpDao(dao, metadata.getSchema());
        metadata.setBinary(dumpDao.isBinary);

        // HDS 4.1.3 - Deletes database records related to awid, which is being imported. The records are deleted according to DAO's ancestor:
        if (!metadata.isBinary()) {
          // HDS 4.1.3.1. - (o) uuObject: deletes records, even the locked ones (DAO deleteMany)
          logger.debug(`Deleting records of uuObject schema ${metadata.getSchema()} for awid ${newAwid}`);
          await dumpDao.deleteManyByAwid(newAwid);
        } else if (!deletedBinaryRegister.includes(daoSchemaName)) {
          // HDS 4.1.3.2. - (o) uuBinary: obtains the list of all uuBinaries (DAO find) and deletes all entries of this list, even locked ones (DAO deleteOne)
          logger.debug(`Deleting records of uuBinary schema ${metadata.getSchema()} for awid ${newAwid}`);
          await dumpDao.deleteBinary({ awid: newAwid }, daoSchemaName);
          deletedBinaryRegister.push(daoSchemaName);
        } else {
          logger.debug(`Records of uuBinary schema ${metadata.getSchema()} already deleted for awid ${newAwid}`);
        }
      } else {
        // HDS 4.2. - (*) For each subsequent row:
        if (metadata.getSchema() === IMPORT_MAPPING_SCHEMA) {
          return;
        }

        // HDS 4.2.1. - Converts data from EJSON format to a document BSON format (A6)
        let document;
        try {
          document = EJSON.parse(line);
        } catch (e) {
          // A6 - The row in a dump is invalid
          throw new Errors.SysImport.InvalidDump({ uuAppErrorMap }, { schema: metadata.getSchema(), cause: e }, e);
        }

        // HDS 4.2.2. - Replaces attribute document.awid by awid (obtained from uri) if it's present in the document
        // (there is nothing to replace for schemas created by MongoDB GridFS)
        if (!metadata.isBinaryAux()) {
          document.awid = newAwid;
        }

        // HDS 4.2.3. - Replaces document._id by newly created ObjectId and saves this relation.
        let newId, oldId;
        // _id of *.files could have been already recreated, so new _id !!CANNOT!! be created again
        if (
          metadata.isBinaryFiles() &&
          fileIdMapping[metadata.getBinarySchema()] &&
          fileIdMapping[metadata.getBinarySchema()][document._id]
        ) {
          newId = fileIdMapping[metadata.getBinarySchema()][document._id];
          oldId = document._id;
          document._id = newId;
        } else {
          newId = this.importMappingDao.getObjectId();
          oldId = document._id;
          document._id = newId;
          // this is technically part of hds 4.2.5, but it has to be here
          mappingsQueue.push({ origId: oldId, newId: newId, origAwid: metadata.getAwid(), awid: newAwid });
        }

        // HDS 4.2.3.1. - (o) For uuBinary schemas replaces also the relations between *.chunks schema (field files_id) *.files schema (field _id)
        // and uuBinary schema itself (field fileId).
        if (metadata.isBinary()) {
          let newIdMapping = this._replaceFileId(metadata, document, fileIdMapping, newId, oldId);
          if (newIdMapping) {
            mappingsQueue.push({
              origId: newIdMapping.orig,
              newId: newIdMapping.new,
              origAwid: metadata.getAwid(),
              awid: newAwid
            });
          }
        }

        // HDS 4.2.4. - If processed document is from sysAppWorkspace schema:
        if (metadata.getSchema() === "sysUuAppWorkspace") {
          await this._handleAppWorkspace(document, appWorkspace);
        }

        // HDS 4.2.5 - Adds modified document to one queue and the relation from HDS 4.2.3. to another queue.
        documentQueue.push({ data: document });

        // HDS 4.2.6., 4.2.7., 4.2.8., A7, A8
        if (sizeOf(documentQueue) > QUEUE_LIMIT) {
          ({ restored, lastLoggedAt } = await this._saveAndLogInsert(
            documentQueue,
            mappingsQueue,
            uuAppErrorMap,
            metadata,
            dumpDao,
            restored,
            lastLoggedAt,
            newAwid
          ));
        }
      }
    });

    // noinspection JSUnusedAssignment
    if (metadata.getSchema() === IMPORT_MAPPING_SCHEMA) {
      return;
    }

    // HDS 4.3. - If the queues are not empty
    if (documentQueue.length !== 0 || mappingsQueue.length !== 0) {
      // HDS 4.3.1., 4.3.2., A9, A10
      // noinspection JSUnusedAssignment
      await this._saveAndLogInsert(
        documentQueue,
        mappingsQueue,
        uuAppErrorMap,
        metadata,
        dumpDao,
        restored,
        lastLoggedAt,
        newAwid,
        true
      );
    }

    // HDS 4.4. - Creates all indexes (metadata.indexList). (DAO createIndex) (A11)
    // noinspection JSUnusedAssignment
    let indexFailures = await dumpDao.importIndexes(metadata.getIndexList());
    if (indexFailures.length !== 0) {
      // A11 - createIndex fails
      if (!uuAppErrorMap[WARNINGS.importIndexImportFailed]) {
        let firstFailure = indexFailures.shift();
        // noinspection SqlNoDataSourceInspection,JSUnusedAssignment
        ValidationHelper.addWarning(uuAppErrorMap, WARNINGS.importIndexImportFailed, "Create index by DAO failed.", {
          rows: [
            {
              schema: metadata.getSchema(),
              index: firstFailure.index,
              cause: firstFailure.cause
            }
          ]
        });
      }
      for (let failure of indexFailures) {
        // noinspection JSUnusedAssignment
        uuAppErrorMap[WARNINGS.importIndexImportFailed].paramMap.rows.push({
          schema: metadata.getSchema(),
          index: failure.index,
          cause: failure.cause
        });
      }
    }
  }

  async _saveAndLogInsert(
    documentQueue,
    mappingsQueue,
    uuAppErrorMap,
    metadata,
    dumpDao,
    restored,
    lastLoggedAt,
    awid,
    force = false
  ) {
    // HDS 4.2.6. - (o) If the size of document queue is more than 32 KB, inserts these documents to the schema using method. (DAO insertMany) (A7)
    // (HDS 4.3.1. - Inserts documents from document queue to the schema. (DAO insertMany) (A9))
    if (documentQueue.length > 0) {
      try {
        await dumpDao.insertMany(documentQueue);
      } catch (e) {
        // A7 (A9) - Inserting the document queue into database fails
        throw new Errors.SysImport.ImportFailed({ uuAppErrorMap }, { schema: metadata.getSchema(), cause: e }, e);
      }
    }

    // HDS 4.2.7. - (o) If the queue is inserted in HDS 4.2.6., inserts the ObjectId relation queue.(SysImportMappingDao create) (A8)
    // (HDS 4.3.2. - Inserts the ObjectId relation queue using. (SysImportMapping DAO create) (A10))
    if (mappingsQueue.length > 0) {
      try {
        await this.importMappingDao.create(mappingsQueue);
      } catch (e) {
        // A8 (A10) - Inserting the relation queue into database fails
        if (e instanceof ObjectStoreError) {
          throw new Errors.SysImport.SysImportMappingDaoCreateFailed({ uuAppErrorMap }, e);
        }
        throw e;
      }
    }

    // HDS 4.2.8. - For every 20% of inserted documents (total count is obtained from metadata) logs the progress at the INFO level.
    // The logging occurs only after the actual database operation and not when the queue is filled.
    restored += documentQueue.length;
    if (force || restored - lastLoggedAt > metadata.getRecordCount() + 0.2) {
      logger.info(
        `Schema: ${metadata.getSchema()}, awid: ${awid}, imported ${restored} out of ${metadata.getRecordCount()} entries`
      );
      lastLoggedAt = restored;
    }

    // this clears the queues
    documentQueue.length = 0;
    mappingsQueue.length = 0;

    return { restored, lastLoggedAt };
  }

  _replaceFileId(metadata, document, fileIdMapping, newId, oldId) {
    // record filled when new fileId was created, i.e. only when generating new ObjectId,
    // which is a foreign key (happen when either chunks or binary itself are processed
    // before files are
    let newIdMapping = null;

    // get the value of fileId, which is about to be replaced
    let origFileId;
    if (metadata.isBinaryFiles()) {
      origFileId = oldId;
    } else if (metadata.isBinaryChunks()) {
      origFileId = document.files_id;
    } else {
      // the uuBinary
      origFileId = document.fileId;
    }

    // check the registry of fileIds mapping
    let key = metadata.getBinarySchema() || metadata.getSchema();
    let newFileId;

    // the registry for this schema was not created yet
    if (!fileIdMapping[key]) {
      fileIdMapping[key] = {};
    }

    if (!fileIdMapping[key][origFileId]) {
      // it is not in the registry (i.e. it was not replaced for either of binary schemas)
      if (metadata.isBinaryFiles()) {
        // fileId is _id for *.files schema => ObjectId was already generated in hds 4.2.3
        newFileId = newId;
      } else {
        // fileId is a foreign key here => new ObjectId is needed
        newFileId = this.importMappingDao.getObjectId();
        newIdMapping = { new: newFileId, orig: origFileId };
      }
      fileIdMapping[key][origFileId] = newFileId;
    } else {
      // fileId is in the registry, i.e. one of the collections already replaced all of the fileIds
      newFileId = fileIdMapping[key][origFileId];
    }

    // set the appropriate fileId
    if (metadata.isBinaryFiles()) {
      document._id = newFileId;
    } else if (metadata.isBinaryChunks()) {
      document.files_id = newFileId;
    } else {
      // the uuBinary
      document.fileId = newFileId;
    }
    return newIdMapping;
  }

  async _handleAppWorkspace(document, appWorkspace) {
    // HDS 4.2.4.1. - Replace document.awidLicenseOwnerList by sysAppWorkspace.awidLicenseOwnerList.
    document.awidLicenseOwnerList = appWorkspace.awidLicenseOwnerList;
    // HDS 4.2.4.2. - Replace document.sysState by sysAppWorkspace.sysState.
    document.sysState = appWorkspace.sysState;
  }
}

module.exports = new SysDumpRestoreAbl();
